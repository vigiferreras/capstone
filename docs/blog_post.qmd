---
title: "The Curse of The Grammys"
subtitle: "Spring 2024"
author: "Virginia Ferreras"
bibliography: references.bib
number-sections: false
format:
  html:
    theme: default
    rendering: embed-resources
    code-fold: true
    code-tools: true
    toc: true
  pdf: default
execute: 
  echo: false
jupyter: python3
---


![Source: Grammys.com](grammy_pic.jpeg)


I am going to list five very different artists and I want you to try and figure out what these artists have in common: Evanescence, Cyndi Lauper, Victoria Monet, Carrie Underwood, and Arrested Development. This is a very challenging task but here is a hint: they have all won a similar music award. If your answer is that they all won a Grammy you are partially correct. These five artists are all recipients of the Grammy’s Best New Artist Award. 

Presented by the Recording Academy of the United States, the Grammy Awards show is one of the most prestigious award shows in the music industry. With over 90 awards recognizing the successes and talents of many individuals within the music industry, the Best New Artist award presented during the show draws a lot of allure due to the fact that it shines light towards talented new artists.

Aside from the excitement and allurement associated with this ceremony and the awards given out, there is a dark mystery that is tied to the Grammys: the Best New Artist Curse. The curse has been a subject of debate for years, stating that winners of this award are cursed with a short-lived career, marked by a decline in successes, achievements, and relevancy. As an avid music listener and fan myself, I have been intrigued by this curse and have always questioned its validity. Do the successes of artists truly decrease after winning this award? Is there a way to predict the success trajectory of future Best New Artist winners? 

To answer these questions I will be utilizing my data science knowledge and skills to analyze the post-award musical achievements of these artists and explore the potential for predicting an artist’s success. Without further-ado, follow me as I combine music and data to dissect the mysteries of the Best New Artist curse. 

```{python}

# libraries used

import pandas as pd
import numpy as np 
import seaborn as sns
import matplotlib.pyplot as plt
import ast

import plotly.graph_objs as go
import plotly.offline as py
import plotly.express as px

from wordcloud import WordCloud
from collections import Counter
import re

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso, Ridge
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn import tree
from sklearn.tree import plot_tree
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score
```

# Data Collection 
Every Data Science project needs its data, therefore I went ahead and collected as much data as I can find publicly to investigate this curse. I went through a meticulous data collection process utilizing various methods:

## Manual Data Entry
Beginning with the focal point of the project, I used the Grammy’s official website to compile a list of Best New Artist winners spanning from 1980 to 2022 [@grammys_info]. To assess post-award activity, I used the IMDb.com website to collect information about music award wins and nominations from the MTV Video Music Awards [@vmas_info] and the American Music Awards [@amas_info], as well as the Recording Industry Association of America (RIAA) website to gather data regarding gold, platinum, and diamond awards [@riaa_info]. For all of these manual collections, I entered the data into a separate spreadsheet for each unique award. 

I also manually collect details about the winners, including birth location, gender, and membership in music groups, primarily from Wikipedia [@wikipedia].

## Web Scraping
To further investigate post-award activity, I employed web-scraping techniques[@web_scraping_help, @web_scraping_help2] to collect Billboard Hot 100s data [@billbord_chart]. Collecting this data will allow us to look for how many chart appearances these award recipients have had post-award win. 

## Spotify API
Utilizing Spotify’s API, I collected data on the genres associated with each Best New Artist winner [@spotify_api]. 

These methods allowed me to collect a wide variety of data that can be used for future analysis. Please keep in mind that there are potential limitations that arise from this collected data, particularly in regards to the reliability of the data collected from public sources. 

# Data Cleaning
``` {python}
#| output: false

best_new = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/best_new2.csv')
best_new_df = pd.DataFrame(best_new)

# Cleaing the Best New Artist dataset

# will be focusing on artists starting from 1985
# dropping the first five columns (1980-1984 winners)
best_new_df = best_new_df.drop(best_new_df.index[:5])

# checking data types
best_new_df.info()

# updating the award_date column data type
best_new_df['award_date'] = pd.to_datetime(best_new_df['award_date'])

# checking the data types again 
best_new_df.info()

# checking for missing values
best_new_df.isna().sum()

# Cleaing the Billboard Hot 100s dataset

hot100_read = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/chart_data.csv')
hot100_df = pd.DataFrame(hot100_read)

# dropping peak position
hot100_df = hot100_df.drop('peak_position', axis = 'columns')

# checking the data types
hot100_df.info()

# adjusting the data type of the week_of column
hot100_df['week_of'] = pd.to_datetime(hot100_df['week_of'])

# checking the data types again
hot100_df.info()

# checking the number of missing values
hot100_df.isna().sum()

# splitting the columns 

# creating a copy of the original hot 100s dataset
hot100_split = hot100_df

# defining the diferrent variations of the word "featuring" 
# This variable will be used as the seperator to split a string of artists
separator_pattern_feat = r'\s+(?:Featuring|feat|ft|Feat|Ft|featuring)\b\s+'

#spliting the column 
split_artists = hot100_split['artist_name'].str.split(separator_pattern_feat, 
                                                      n = 1, 
                                                      expand = True)

# Adding the new split columns to the hot100_split dataset 
hot100_split['main_artist'] = split_artists[0]
hot100_split['featured_artists'] = split_artists[1]

# new variable to define the diferrent variations of the word "and"
# includes the exceptions of specific words from groups that shouldn't be split
separator_pattern_main = r'(?i)\s+(?:&(?! The Range| The Blowfish| Ryan Lewis)|and(?! The Range| The Blowfish| Ryan Lewis))\s+'


# splitting the 'main_artist' column based on the separator pattern
split_main_artists = hot100_split['main_artist'].str.split(separator_pattern_main, expand = True)

# renaming the split columns
split_main_artists.columns = [f'main_artist_{i + 1}' for i in range(split_main_artists.shape[1])]

# concatenate the split columns with the original DataFrame
# creating a new df to avoid confusion and record each change made
hot100_split2 = pd.concat([hot100_split, split_main_artists], axis = 1)

# dropping two redunant columns
hot100_split2 = hot100_split2.drop(['artist_name', 'main_artist'], axis = 1)

# reordering the remaining columns
hot100_split2 = hot100_split2.iloc[:, [0,1,2,4,5,6,3]]

# new variable to define the different variations of the word "and"
separator_pattern_and = r'\s+(?:&|and|And|,)\s+'

# splitting the featured_artists column
split_artists = hot100_split2['featured_artists'].str.split(separator_pattern_and, expand = True)

# renaming the split columns
split_artists.columns = [f'featured_artist_{i + 1}' for i in range(split_artists.shape[1])]

# concatenating the split columns with the previous split dataset 
# creating a new df to avoid confusion and record each change made
hot100_split3 = pd.concat([hot100_split2, split_artists], axis = 1)

# dropping the featured artist column 
hot100_split3 = hot100_split3.drop(['featured_artists'], axis = 1)

# the seperator in this split is just a comma
split_feats = hot100_split3['featured_artist_1'].str.split(',', expand = True)

# renaming the split columns
split_feats.columns = [f'featured_artist_{i + 4}' for i in range(split_feats.shape[1])]

# concatenating the split columns with the previous split dataset
# creating a new df to avoid confusion and record each change made
hot100_split4 = pd.concat([hot100_split3, split_feats], axis = 1)

# dropping the featured_artist_1 column
hot100_split4 = hot100_split4.drop('featured_artist_1', axis = 'columns')

# reordering the columns for easier readability
hot100_split4 = hot100_split4.iloc[:, [0,1,2,3,
                                       4,5,8,9,
                                       10,11,6,7,
                                       12,13,14,15]]

# renaming column headers
hot100_split4.columns = ['chart_week', 'rank', 'song_title', 'main_artist_1', 'main_artist_2', 'main_artist_3',
                       'featured_artist_1', 'featured_artist_2', 'featured_artist_3', 'featured_artist_4', 
                        'featured_artist_5', 'featured_artist_6', 'featured_artist_7', 'featured_artist_8',
                       'featured_artist_9', 'featured_artist_10']

# applying the strip() function to values in each object labeled column
for col in hot100_split4.iloc[:, 2:]:
    hot100_split4[col] = hot100_split4[col].str.strip()

# Best New Award Winners
selected_artists = best_new_df['artist'].unique()

# Filtering the hot100_split4 DataFrame to include rows where any of the selected artists appear
filtered_hot100 = hot100_split4[
    (hot100_split4['main_artist_1'].isin(selected_artists)) |
    (hot100_split4['main_artist_2'].isin(selected_artists)) |
    (hot100_split4['main_artist_3'].isin(selected_artists)) |
    (hot100_split4['featured_artist_1'].isin(selected_artists)) |
    (hot100_split4['featured_artist_2'].isin(selected_artists)) |
    (hot100_split4['featured_artist_3'].isin(selected_artists)) |
    (hot100_split4['featured_artist_4'].isin(selected_artists)) |
    (hot100_split4['featured_artist_5'].isin(selected_artists)) |
    (hot100_split4['featured_artist_6'].isin(selected_artists)) |
    (hot100_split4['featured_artist_7'].isin(selected_artists)) |
    (hot100_split4['featured_artist_8'].isin(selected_artists)) |
    (hot100_split4['featured_artist_9'].isin(selected_artists)) |
    (hot100_split4['featured_artist_10'].isin(selected_artists))
]

# filtered_hot100s copy
filtered_hot100_copy = filtered_hot100

# making new columns for main_artist and featured_artist
filtered_hot100_copy['main_artist'] = 'no' # default = no 
filtered_hot100_copy['featured_artist'] = 'no' # default = no 
filtered_hot100_copy['artist'] = None  # default = None

# iterating through each row in filtered_hot100_copy
for index, row in filtered_hot100_copy.iterrows():
    
    # checking if any of the selected artists are in columns 4, 5, or 6
    if any(artist in row.values[3:6] for artist in selected_artists):
        filtered_hot100_copy.at[index, 'main_artist'] = 'yes' # marking yes
    else:
        filtered_hot100_copy.at[index, 'featured_artist'] = 'yes'  # marking yes
    
    # assigning the name of the searched artist to the new column
    for artist in selected_artists:
        if artist in row.values[3:]: 
            #if the artists is found in any column, assign the name of the artist to new column
            filtered_hot100_copy.at[index, 'artist'] = artist
            break  # stop searching once the artist is found

# reordering the columns 
filtered_hot100_copy = filtered_hot100_copy.iloc[:, [18,0,1,2,16,17]]

# creating the boxplot
plt.figure(figsize=(8, 6))
sns.boxplot(data=filtered_hot100_copy, x='rank', orient='h', linewidth=2, color='lavender')
plt.title("Box Plot For Hot 100's Rank Column")
plt.ylabel('Rank')
plt.show()


# Cleaning the Artists Genre dataset 

artist_genres = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/artist_general_info.csv')
artist_genres_df = pd.DataFrame(artist_genres)

# similar to the best_new_df, will be dropping the first five rows
artist_genres_df = artist_genres_df.drop(artist_genres_df.index[:5])
artist_genres_df = artist_genres_df.reset_index()

# dropping the three unneccesary columns
artist_genres_df = artist_genres_df.drop(['followers', 'popularity', 'index'], axis = 'columns')

# removing the brackets using string slicing 
artist_genres_df['genre'] = artist_genres_df['genre'].str[1:-1]

# removing the quotes
artist_genres_df['genre'] = artist_genres_df['genre'].str.replace("'", "")

artist_genres_df.info()

# Cleaning the MTV Video Music Awards Dataset

vmas_awards = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/vmas_winners_nominees2.csv')
vmas_awards_df = pd.DataFrame(vmas_awards)

# making copy of the original vmas_awards_df
vmas_copy = vmas_awards_df

vmas_split1 = vmas_copy['artist_name'].str.split('Feat.|feat.|Feat|feat', n = 1, expand = True)

vmas_copy['main_artist'] = vmas_split1[0]

vmas_copy['featured_artists'] = vmas_split1[1]

vmas_split2 = vmas_copy['main_artist'].str.split(r'(?<!Tyler),\s*', expand = True)

vmas_split2.columns = [f'main_artist_{i+1}' for i in range(vmas_split2.shape[1])]

# Concatenate the split columns with the original DataFrame
vmas_copy = pd.concat([vmas_copy, vmas_split2], axis=1)

vmas_copy.reset_index(drop=True, inplace=True)

vmas_split3 = vmas_copy['featured_artists'].str.split(',', expand = True)

vmas_split3.columns = [f'featured_artist_{i+1}' for i in range(vmas_split3.shape[1])]

# concatenating the split columns with the original df
vmas_copy = pd.concat([vmas_copy, vmas_split3], axis=1)

# dropping the three nnnecessary columns
vmas_copy = vmas_copy.drop(['artist_name', 'featured_artists', 'main_artist'], axis = 'columns')

# removing the possible white spaces the strings have from splitting
for col in vmas_copy.iloc[:, 3:]:
    vmas_copy[col] = vmas_copy[col].str.strip()

# best New Award Winners
selected_artists = best_new_df['artist'].unique()

# filtering the hot100_split4 DataFrame to include rows where any of the selected artists appear
filtered_vmas = vmas_copy[
    (vmas_copy['main_artist_1'].isin(selected_artists)) |
    (vmas_copy['main_artist_2'].isin(selected_artists)) |
    (vmas_copy['main_artist_3'].isin(selected_artists)) |
    (vmas_copy['main_artist_4'].isin(selected_artists)) |
    (vmas_copy['main_artist_5'].isin(selected_artists)) |
    (vmas_copy['featured_artist_1'].isin(selected_artists)) |
    (vmas_copy['featured_artist_2'].isin(selected_artists)) |
    (vmas_copy['featured_artist_3'].isin(selected_artists)) |
    (vmas_copy['featured_artist_4'].isin(selected_artists)) |
    (vmas_copy['featured_artist_5'].isin(selected_artists)) |
    (vmas_copy['featured_artist_6'].isin(selected_artists)) 
]

# making a copy of the filtered vmas dataset 
filtered_vmas_copy = filtered_vmas

# creating new columns for main_artist and featured_artist
filtered_vmas_copy['main_artist'] = 'no'
filtered_vmas_copy['featured_artist'] = 'no'
filtered_vmas_copy['artist'] = None  # Initialize new column with None values

# iterating over each row in filtered_hot100
for index, row in filtered_vmas_copy.iterrows():
    # checking if any of the selected artists are in columns 4, 5, 6, 7, 8
    if any(artist in row.values[3:9] for artist in selected_artists):
        filtered_vmas_copy.at[index, 'main_artist'] = 'yes'
    else:
        filtered_vmas_copy.at[index, 'featured_artist'] = 'yes'
    
    # assigning the name of the searched artist to the new column
    for artist in selected_artists:
        if artist in row.values[3:]:
            filtered_vmas_copy.at[index, 'artist'] = artist
            break  # end search once the artist is found

# reordering the columns and excluding main/featured split columns
filtered_vmas_copy = filtered_vmas_copy.iloc[:, [16,0,1,2,14,15]]

filtered_vmas_copy.info()

# stripping white spaces from win column
filtered_vmas_copy['win'] = filtered_vmas_copy['win'].str.strip()

# fixing the data type for the date column
filtered_vmas_copy['date'] = pd.to_datetime(filtered_vmas_copy['date'])

# Cleaning the American Music Awards Dataset

amas_awards = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/ama_wins_nominees2.csv')
amas_awards_df = pd.DataFrame(amas_awards)

# copying the amas_awards_df
amas_copy = amas_awards_df

# splitting the artist name by commas
amas_split = amas_copy['artist_name'].str.split(',', expand = True)

amas_split.columns = [f'artist_{i + 1}' for i in range(amas_split.shape[1])]

# concatenating the split columns with the original DataFrame
amas_copy = pd.concat([amas_copy, amas_split], axis=1)

# dropping the artist_name column 
amas_copy = amas_copy.drop('artist_name', axis = 1)

amas_copy.info()

# removing the possible white spaces the strings have from splitting
for col in amas_copy.iloc[:, 3:]:
    amas_copy[col] = amas_copy[col].str.strip()

# Best New Award Winners
selected_artists = best_new_df['artist'].unique()

# filtering the ama_copy DataFrame to include rows where any of the selected artists appear
filtered_amas = amas_copy[
    (amas_copy['artist_1'].isin(selected_artists)) |
    (amas_copy['artist_2'].isin(selected_artists)) |
    (amas_copy['artist_3'].isin(selected_artists)) |
    (amas_copy['artist_4'].isin(selected_artists)) |
    (amas_copy['artist_5'].isin(selected_artists)) |
    (amas_copy['artist_6'].isin(selected_artists))

]

# making a copy of the filtered_amas df
filtered_amas_copy = filtered_amas

# creating new columns for main_artist and featured_artist
filtered_amas_copy['artist'] = None  # initializing new column with None values

# iterating over each row in filtered_hot100
for index, row in filtered_amas_copy.iterrows():
    for artist in selected_artists:
        if artist in row.values[3:]:
            filtered_amas_copy.at[index, 'artist'] = artist
            break  # end search once the artist is found

# reordering the columns
filtered_amas_copy = filtered_amas_copy.iloc[:, [9,0,1,2]]

# adjusting the data type for the date column
filtered_amas_copy['date'] = pd.to_datetime(filtered_amas_copy['date'])

# stripping white spaces from win column
filtered_amas_copy['win'] = filtered_amas_copy['win'].str.strip()

filtered_amas_copy.info()

# Cleaning Recording Industry Association of America (RIAA) Awards dataset

riaa_awards = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/RIAA_awards.csv')
riaa_awards_df = pd.DataFrame(riaa_awards)

# making a copy of the riaa_awards_df
riaa_copy = riaa_awards_df

# Changing months to number format

month_num = {
    'January': '01',
    'February': '02',
    'March': '03',
    'April': '04',
    'May': '05',
    'June': '06',
    'July': '07',
    'August': '08',
    'September': '09',
    'October': '10',
    'November': '11',
    'December': '12'
}

riaa_copy['award_month'] = riaa_copy['award_month'].map(month_num)

riaa_copy.info()

# changing the data type for the date column 
riaa_copy['award_month'] = pd.to_numeric(riaa_copy['award_month'])

# combining the month, day, year column to make one date column 

# combining the columns to create a new date column
riaa_copy['award_date'] = riaa_copy['award_year'].astype(str) + '-' + riaa_copy['award_month'].astype(str) + '-' + riaa_copy['award_day'].astype(str)

# converting the new date column to datetime format
riaa_copy['award_date'] = pd.to_datetime(riaa_copy['award_date'])

# dropping the original columns
riaa_copy = riaa_copy.drop(['award_year', 'award_month', 'award_day'], axis=1)

# replacing any occurrence of '[number]x Platinum' with 'Multi Platinum'
riaa_copy['award_type'] = riaa_copy['award_type'].str.replace(r'^(\d+)x\s*Platinum$', 'Multi Platinum', regex = True)

# replacing any occurrence of '[number]x Diamond' with 'Multi Diamond'
riaa_copy['award_type'] = riaa_copy['award_type'].str.replace(r'^(\d+)x\s*Diamond$', 'Multi Diamond', regex = True)

# making dummy columns for future analysis

dummy_cols = ['award_type']
riaa_dummies = pd.get_dummies(riaa_copy[dummy_cols], prefix = dummy_cols)
dummy_riaa_df = pd.concat([riaa_copy, riaa_dummies], axis = 1)

# dropping the award type column
dummy_riaa_df = dummy_riaa_df.drop('award_type', axis = 'columns')


# changing headers
dummy_riaa_df = dummy_riaa_df.rename(columns = {'award_type_Gold': 'gold',
                                                'award_type_Platinum': 'platinum',
                                                'award_type_Diamond': 'diamond',
                                                'award_type_Multi Diamond': 'multi_diamond',
                                                'award_type_Multi Platinum': 'multi_platinum',})

# stripping white spaces from artist column
dummy_riaa_df['artist'] = dummy_riaa_df['artist'].str.strip()

dummy_riaa_df.info()

# Cleaning Artist Information dataset 

artist_info = pd.read_csv('/Users/virginiaferreras/Desktop/capstone/data/artist_info_2.csv')
artist_info_df = pd.DataFrame(artist_info)

# stripping the white spaces from the gender column
artist_info_df['gender'] = artist_info_df['gender'].str.lower().str.strip()


# stripping the white spaces from the music group column
artist_info_df['music_group'] = artist_info_df['music_group'].str.lower().str.strip()

artist_info_df.info()
```

With any data driven project, data preprocessing is a crucial step in creating your models and visualizations. For this project I went through each dataset and  performed the usual preprocessing techniques which included: 

- Changing data types: I had to adjust any date column on my datasets to be a date/time object rather than a categorical object

- Checking for Missing and Duplicate Values: Luckily, since I mainly collected the data manually and had full control over what data I was grabbing, I had no missing or duplicated values to worry about!
 
- Dropping unnecessary columns: After collecting the data, I had time between collecting it and creating my code to really think about the practicality some columns had towards my overall project. Therefore some columns were dropped during this stage.

- Removing white spaces, brackets, changing casing: Very typical grammar fixes

Now aside from the simple techniques, I did have to do a little more thinking and perform some complicated procedures. First and foremost, many songs that are featured on the charts or nominated for an award may have either more than one mian artist or more than one featured artist on the song. Since my project is focusing on the post-award win activity of each of the winners, I want to ensure I include every instance an artist is seen on a chart or award nomination, even if that means they are not the only artist on that song. Therefore, I had to split the artist columns on the Billboard Hot 100s, VMA, and AMA datasets in order to have each artist on a song be represented by their own column. After separating each of the main and featured artists, I went ahead and filtered the dataset to only include the instances where the Best New Artists winners were spotted and reduced to have two binary columns that verify whether the artist is a main artist or featured artist for this instance. The VMA and AMA dataset also had a column that included whether they won the award or not. 

The RIAA dataset also had me perform some extra steps to get it cleaned up. Aside from adjusting and combing the separated date columns, I also went ahead and renamed some of the award names. Any platinum or diamond award that has a value such as “2x, 3x, 4x, etc” in front of it was renamed to simply “multi-platinum” or “multi-diamond”. I did this so that it will be easier to visualize and understand. Furthermore, I created dummy variables for each of the awards types. 

Lastly the Artist Genre dataset also had some extra steps to take care of during this stage. It should be well known that which the fast growth of music throughout the years, there has been an expansion of genres artists sing. In fact, many artists may be associated with more than one  genre. Due to this, for each artist the Spotify API gave me a list of genres associated with this artist. Now while it is important for artists to be labeled by their respective genre, in this project the subgenres are a little too specific. Therefore, the EDA section of my project will utilize the list of genres for each artist while the modeling part of the project I made the decision to generalize the sub genres they are associated with to the main genre they fall under. After adjusting their associated genre, I performed one-hot encoding to split each genre into a separate column in preparation for future modeling. 

# Exploratory Data Analysis

## Patterns Among Best New Artist Winners

Before delving into the analysis of these winners' success, lets breifly examine the commonalities shared by these artists. Are there any noticable patterns among the features shared by Best New Artist winners?

```{python}

#| label: fig-gender_distribution
#| fig-cap: "Distribution of the Best New Artist winners by gender." 
#| echo: true

# counting the number of wins of each gender
gender_counts = artist_info_df['gender'].value_counts()

# creating the  bar chart
plt.figure(figsize=(6, 4))

gender_counts.plot(kind='bar', color=['mediumpurple','lightskyblue','burlywood'])

plt.title('Grammys Best New Artist Award by Gender', 
          fontsize = 15, 
          fontweight = 'bold')

plt.xlabel('Gender', fontsize = 12, 
           fontweight='bold')

plt.ylabel('Number of Awards', fontsize = 12, 
           fontweight='bold')

plt.xticks(rotation=0)  # Rotate x-axis labels if necessary
plt.tight_layout()

plt.show()
```

```{python}

#| label: fig-group_distribution
#| fig-cap: "Distribution of the Best New Artist winners in Music Groups." 
#| echo: true

# counting the number of music groups
group_counts = artist_info_df['music_group'].value_counts()

# creating the bar chart
plt.figure(figsize=(6, 4))

group_counts.plot(kind='bar', color=['firebrick', 'mediumblue'])

plt.title('Distribution of Music Groups', 
          fontsize = 15, 
          fontweight = 'bold')

plt.xlabel('Music Group', 
           fontsize = 12, 
           fontweight = 'bold')

plt.ylabel('Count', 
           fontsize = 12, 
           fontweight = 'bold')
plt.xticks(rotation=0)  # Rotate x-axis labels if necessary
plt.tight_layout()

plt.show()
```

According to the @fig-gender_distribution, it is evident that female artists dominate the Grammys' Best New Artist award, followed by male artists. The "both" catergory represents bands that includes memebrs of both genders. Similarly, @fig-group_distribution tells us that solo artists tend to win these awards over music groups. 

While this observation does not relate to my initial research question, it is interesting to note the patterns in who these winners tend to be. 


## Most Popular Genres Among Best New Artist Winners 
```{python}

#| label: fig-word_cloud
#| fig-cap: "Timeline of when each Best New Artist won or was nominated for a VMA or AMA award" 
#| echo: true

# creating word cloud to visualize genre popularity

# replacing any instance of hip hop with hip-hop. This will help keep the words connected
artist_genres_df['genre'] = artist_genres_df['genre'].str.replace('hip hop', 'hip-hop')

# combining the genre row into one single string and ignoring the commas
text_data = ' '.join(artist_genres_df['genre'].str.replace(',', '').values)

# ensuring hiphop is connected 
text_data = text_data.replace('hip-hop', 'hiphop')

# & is not picked up in the word cloud
# renaming r&b to rhythmblues to ensure it is represented properly in the word cloud
text_data = text_data.replace('r&b', 'rhythmblues')

# ensuring neo soul is connected
text_data = text_data.replace('neo soul', 'neosoul')

# combining the genre row into one single string,
# removing commas too
text_data = ' '.join(artist_genres_df['genre'].str.replace(',', '').values)

# combining the words in each genre string into a single string,
# removing spaces, hyphens and replacing "r&b" with "rhythmblues"
text_data = ' '.join(artist_genres_df['genre'].str.replace(' ', '').str.replace('', '').str.replace('-', '').str.replace('r&b', 'rhythmblues').str.replace(',', ' ').values)

# creating the wordcloud 
wordcloud = WordCloud(width = 800, 
                      height = 400, 
                      background_color = 'white', 
                      colormap = 'twilight', 
                      random_state = 30).generate(text_data)

# Display the word cloud
plt.figure(figsize = (8, 6))
plt.imshow(wordcloud, 
           interpolation = 'bilinear')
plt.axis('off')

plt.show()
```

Next up, since each artist is associated with several different genres, I want to visualize the most common genres that are shared by these Best New Artist winners. While this information is not directly appicable to my research question, it is interesting to see what genres tend to dominant the Best New Artist award. Based on @fig-word_cloud, we can see that some of the most popular genres include: dance pop, pop, urban contemporary, new wave pop, and rhythm and blues (R&B). 

## Artist Activity Throughout the Years
In order to really investigate whether the success of an artist decreases after winning the Best New Artist award. I created two x-axis interactive timeline charts, one chart visualizes when each artist appeared on the Billboard Hot100s chart and the other chart visualizes when each artist won/were nominated for an award.

These timelines will allow us to view how active and relevant each artist was after winning their Best New Artist Award.

```{python}

#| label: fig-vma_ama
#| fig-cap: "Timeline depicting when each Best New Artist won or was nominated for a MTV Video Music Award (VMA) or American Music Award (AMA) in relation to their Grammy Best New Artist win." 
#| warning: false

# creating a plotly figure
fig = go.Figure()

# this part will create the info I want to display in the hover text box for each dataset I use

# creating empty list for VMAs
hover_text_vmas = []

# interating through filtered vmas df
for index, row in filtered_vmas_copy.iterrows():
    
    # checking win column for each artist
    outcome = 'Winner' if row['win'] == 'Yes' else 'Nominee'
    
    date = row['date'].strftime("%Y-%m-%d")
    
    # concatenating the information in the hover text
    text = f"Artist Name: {row['artist']}<br>" \
       f"Award Name: {row['award_category']}<br>" \
       f"Date: {date}<br>" \
       f"Outcome: {outcome}"
    
    # appending text to list
    hover_text_vmas.append(text)

# creating empty list for AMAs    
hover_text_amas = []
for index, row in filtered_amas_copy.iterrows():
    
    # checking win column for each artist
    outcome = 'Winner' if row['win'] == 'Yes' else 'Nominee'
    
    date = row['date'].strftime("%Y-%m-%d")
    
    # concatenating the information in the hover text
    text = f"Artist Name: {row['artist']}<br>" \
          f"Award Name: {row['award_name']}<br>" \
          f"Date: {date}<br>" \
          f"Outcome: {outcome}"
    
    
    # appending text to list
    hover_text_amas.append(text)    

# creating empty list for Best New Artist win
hover_text_best = []
for index, row in best_new_df.iterrows():
    
    date = row['award_date'].strftime("%Y-%m-%d")
    
    # no need to check wins, as these winners already did win the award
    text = f"Best New Artist: {row['artist']}<br>" \
           f"Date: {date}<br>" 
    
    # appending text to list
    hover_text_best.append(text)        
    
# creating the timeline 

# scatter markers for Best New Artist Win
fig.add_trace(go.Scatter(x = best_new_df['award_date'], 
                         y = best_new_df['artist'],
                         mode = 'markers', 
                         marker = dict(line = dict(width = 2,
                                                color = 'black'),
                                     symbol = 'star', 
                                     size = 20, 
                                     color = 'gold'),
                         name ='Best New Artist Win', 
                         hoverinfo ='text',
                         text = hover_text_best))


# scatter markers for VMA awards
fig.add_trace(go.Scatter(x = filtered_vmas_copy['date'], 
                         y = filtered_vmas_copy['artist'],
                         mode = 'markers', 
                         marker = dict(line = dict(width = 1),
                                     symbol = 'circle', 
                                     size = 10, 
                                     color = 'darkorange'),
                         opacity = 0.9,
                         name = 'VMA Win or Nomination', 
                         hoverinfo = 'text',
                         text = hover_text_vmas))

# scatter markers for AMA awards
fig.add_trace(go.Scatter(x = filtered_amas_copy['date'],
                         y = filtered_amas_copy['artist'],
                         mode = 'markers', 
                         marker = dict(line = dict(width = 1),
                                     symbol = 'circle', 
                                     size = 10, 
                                     color = 'limegreen'),
                         opacity = 0.9,
                         name = 'AMA Win or Nomination', 
                         hoverinfo = 'text',
                         text = hover_text_amas))

fig.update_layout(width = 800, 
                  height = 600, 
                  plot_bgcolor='lightgray',
                  title='Awards Received by Artists',
                  title_font=dict(size=20, family='Arial, sans serif'),
                  xaxis=dict(title='Date',
                             title_font=dict(size=15, family='Helvetica, sans-serif'),
                             tickfont=dict(size=10)),
                  yaxis=dict(title='Artist',
                             title_font=dict(size=15, family='Helvetica, sans-serif'),
                             tickfont=dict(size=10),
                             autorange="reversed"),
                  legend=dict(font=dict(size=10))
                 )

py.iplot(fig)
```

``` {python}

#| label: fig-hot100s_timeline
#| fig-cap: "Timeline of Best New Artist wins and Billboard Hot 100s entries." 
#| warning: false

fig = go.Figure()

hover_text_best = []
for index, row in best_new_df.iterrows():
    
    date = row['award_date'].strftime("%Y-%m-%d")
    
    # no need to check wins, as these winners already did win the award
    text = f"Best New Artist: {row['artist']}<br>" \
           f"Date: {date}<br>" 
    
    # appending text to list
    hover_text_best.append(text)  

# scatter markers for Best New Artist Win
fig.add_trace(go.Scatter(x = best_new_df['award_date'], y = best_new_df['artist'],
                         mode = 'markers', marker = dict(line = dict(width = 2,
                                                                 color = 'black'),
                                                     symbol = 'star', 
                                                     size = 20, 
                                                     color = 'gold'),
                         name = 'Best New Artist Win', 
                         hoverinfo = 'text',
                         text = hover_text_best))


hover_text_hot100 = []
for index, row in filtered_hot100_copy.iterrows():
    
    date = row['chart_week'].strftime("%Y-%m-%d")
    
    # no need to check wins, as these winners already did win the award
    text = f"Artist Name: {row['artist']}<br>" \
           f"Billboard Hot 100s Chart Week: {date}<br>" 
    
    # appending text to list
    hover_text_hot100.append(text)  
    
# scatter markers for hot 100s df
fig.add_trace(go.Scatter(x = filtered_hot100_copy['chart_week'], 
                         y = filtered_hot100_copy['artist'],
                         mode ='markers', 
                         opacity = 0.7,
                         marker = dict(symbol = 'circle', 
                                     size = 10, 
                                     color = 'purple'),
                         name = 'Charted Song',
                         hoverinfo = 'text',
                         text = hover_text_hot100))

fig.update_layout(width = 800, 
                  height = 600, 
                  plot_bgcolor = 'lightgray',
                  title = 'Billboard Hot 100s Charted Songs',
                  title_font = dict(size = 20),
                  xaxis = dict(title = 'Date', 
                               title_font = dict(size = 15, 
                                                 family = 'Helvetica, sans-serif'), 
                               tickfont = dict(size = 10)),
                  yaxis = dict(title = 'Artist',
                               title_font = dict(size = 15, 
                                                 family = 'Helvetica, sans-serif'), 
                               tickfont = dict(size = 10),
                               autorange ="reversed"),
                  legend = dict(font = dict(size = 10))
                 )

py.iplot(fig)
```

Both timelines yield very similar conclusions that challenged the speculation that winning the Best New Artist award leads to an unsuccessful or inactive careers. @fig-vma_ama displays the award and nomination activity for each artist along with the date they won the Best New Artist Award, which offers us insights inot their post-Grammy recognition. Reviewing both wins and nominations allows us to figure out if these artists continue to be celebrated for their music. While some artists indeed fade away from prominence after their Grammy win (such as Jody Watley, Esperanza Spalding, Macklemore & Ryan Lewis), others contine to receive acclaim (such as Mariah Carey, Alicia Keys, Carrie Underwood, Maroon 5). 

@fig-hot100s_timeline is my personal favorite. From the data I collected, in my opinion the Billboard Hot 100s data is a very good and reliable indicator of how well an artist is doing in the music industry. This chart is a very respectable chart that many artists yearn to appear on [@chart_important]. Being on this chart means that an artist's song is very popular during the week, taking into account its sales, streams, radio play, and online downloads. 

This timeline shows us that a lot of artists had very long and successful careers post-award win. For example Mariah Carey can be seen to have a very long and consistent career after being named the Best New Artist in 1991. This timeline shows us that not every artist’s career deteriorated after winning the Best New Artist Award. Many artists are either still relevant today, as they are still being seen on the charts, or have generally had a long career, such as Toni Braxton and Christina Aguilera. While there are artists who did suffer landing on the charts post win, this may not be the work of the Best New Artist curse, rather it may just be due to coincidence or more personal reasons. This timeline helps prove that this curse is simply just a myth, and that not all of the artists who win the award suffer career decline. 

:::{.callout-tip}
## Tip: How to read the timelines
- Each line on the timeline represents a different artists
- The star represents when an artist won the Best New Artist award
- The purple dot in @fig-hot100s_timeline represents an instance when the artists appears on the Billboard Hot 100s chart
- The green dot in @fig-vma_ama represents when an artist won an award they were nominated for, and the orange dot represents when an artist was nominated for an award. 
:::

# Predicting Success
Despite the limitations of a fairly small dataset, I want to attempt to make a model that can predict the success of an artist after winning the Best New Artist Award. Now, you might be wondering, how exactly am I going to predict the success of an artist? Well the answer lies in creating my own unique success metric.

## Quantifying Success
``` {python}

# Artists excluded due to lack of information
excluded_artists = ['Olivia Rodrigo', 'Megan Thee Stallion', 'Billie Eilish', 'Dua Lipa']

# Calculating the Billboard Hot 100s Score

# merging 'filtered_hot100_copy' with 'best_new_df' on the 'artist' column
merged_df = pd.merge(filtered_hot100_copy, best_new_df, on='artist', how='inner')

# filtering the singles within five years after the award date
filtered_df = merged_df[(merged_df['chart_week'] >= merged_df['award_date']) &
                        (merged_df['chart_week'] <= merged_df['award_date'] + pd.DateOffset(years=5))]

billboard_scores_df = pd.DataFrame(columns = ['artist', 'billboard_score'])

# score variable 
score = 0 

for index, row in filtered_df.iterrows():
    
    # this if-else statement represents whether the artist was a main artist or not
    if row['main_artist'] == 'yes':
        
        # these if-else statements represents the chart rank position of the song
        if 1 <= row['rank'] <= 20:
            score += 5
        
        elif 21 <= row['rank'] <= 40:
            score += 4
        
        elif 41 <= row['rank'] <= 60:
            score += 3
        
        elif 61 <= row['rank'] <= 80:
            score += 2
        
        else:
            score += 1
            
    else:
        score += 1
    
    # adding the score to the df
    if row['artist'] in billboard_scores_df['artist'].values:
        
        billboard_scores_df.loc[billboard_scores_df['artist'] == row['artist'], 'billboard_score'] += score
    
    # adding artist in df if they are not already there, adding score too    
    else:
        new_row = pd.DataFrame({'artist': [row['artist']], 'billboard_score': [score]})
        billboard_scores_df = pd.concat([billboard_scores_df, new_row], ignore_index = True)
        
    # resetting score variable
    score = 0   

# merging wihth the best_new_df to display all the Best New Artists
merged_billboard_scores = pd.merge(best_new_df, billboard_scores_df, on = 'artist', how = 'left')

# filling in missing values with 0
merged_billboard_scores['billboard_score'] = merged_billboard_scores['billboard_score'].fillna(0)

# changing billboard score column data type to integer

merged_billboard_scores['billboard_score'] = merged_billboard_scores['billboard_score'].astype('int64')

# creating df without the excluded artists
filter_billboard_scores = merged_billboard_scores[~merged_billboard_scores['artist'].isin(excluded_artists)]

filter_billboard_scores = filter_billboard_scores.reset_index()

filter_billboard_scores = filter_billboard_scores.drop(columns = 'index')

# VMAS Sccore

# merging 'filtered_vmas_copy' with 'best_new_df' on the 'artist' column
merged_vmas = pd.merge(filtered_vmas_copy, best_new_df, on='artist', how='inner')


# filtering the singles within five years after the award date
five_yr_vmas = merged_vmas[(merged_vmas['date'] >= merged_vmas['award_date']) &
                        (merged_vmas['date'] <= merged_vmas['award_date'] + pd.DateOffset(years=5))]

# checking to see if any artists are condiered featured artists in the df
five_yr_vmas['featured_artist'].unique()

# counting the number of wins and losses
vmas_count = five_yr_vmas.groupby(['artist', 'win']).size().unstack(fill_value = 0)

vmas_count.columns = ['vmas_lost', 'vmas_win']

vmas_count.reset_index()

# merging the count with the best_new_df

counts_vmas_merge = pd.merge(best_new_df, vmas_count, on = 'artist', how = 'left')

counts_vmas_merge['vmas_lost'] = counts_vmas_merge['vmas_lost'].fillna(0).astype('int64')

counts_vmas_merge['vmas_win'] = counts_vmas_merge['vmas_win'].fillna(0).astype('int64')

# creating dd without the excluded artists
filter_vmas_five = counts_vmas_merge[~counts_vmas_merge['artist'].isin(excluded_artists)]

filter_vmas_five = filter_vmas_five.reset_index()

filter_vmas_five = filter_vmas_five.drop(columns = 'index')

filter_vmas_five['vmas_lost'] = filter_vmas_five['vmas_lost']

filter_vmas_five['vmas_win'] = filter_vmas_five['vmas_win']


# finding overall score
vmas_score_list = []

for index, row in filter_vmas_five.iterrows():
    
    artist_score = (row['vmas_lost'] * 2) + (row['vmas_win'] * 5)
    vmas_score_list.append(artist_score)
    
    
filter_vmas_five['vmas_score'] = vmas_score_list

# AMAS Score

# merging 'filtered_amas_copy' with 'best_new_df' on the 'artist' column
merged_amas = pd.merge(filtered_amas_copy, best_new_df, on='artist', how='inner')


# filtering the singles within five years after the award date
five_yr_amas = merged_amas[(merged_amas['date'] >= merged_amas['award_date']) &
                        (merged_amas['date'] <= merged_amas['award_date'] + pd.DateOffset(years=5))]

# counting wins and losses
amas_count = five_yr_amas.groupby(['artist', 'win']).size().unstack(fill_value = 0)

amas_count.columns = ['amas_lost', 'amas_win']

# merging the best_new df with the wins/losses count df 
counts_amas_five = pd.merge(best_new_df, amas_count, on = 'artist', how = 'left')

# filled in missing values with 0
counts_amas_five['amas_lost'] = counts_amas_five['amas_lost'].fillna(0)
counts_amas_five['amas_win'] = counts_amas_five['amas_win'].fillna(0)

# creating df without the excluded artists
filter_amas_five = counts_amas_five[~counts_amas_five['artist'].isin(excluded_artists)]

filter_amas_five = filter_amas_five.reset_index()

filter_amas_five = filter_amas_five.drop(columns = 'index')

# adjusting data types
filter_amas_five['amas_lost'] = filter_amas_five['amas_lost'].astype('int64')
filter_amas_five['amas_win'] = filter_amas_five['amas_win'].astype('int64')

# totaling up the score

amas_score_list = []

for index, row in filter_amas_five.iterrows():
    artist_score = (row['amas_lost'] * 2) + (row['amas_win'] * 5)
    amas_score_list.append(artist_score)
    
    
filter_amas_five['amas_score'] = amas_score_list

# RIAA Score

# merging 'dummy_riaa_df' with 'best_new_df' on the 'artist' column
merged_riaa = pd.merge(dummy_riaa_df, best_new_df, on='artist', how='inner', suffixes = ('_riaa', '_bestnew'))


# filtering the songs within five years after the award date
five_yr_riaa = merged_riaa[(merged_riaa['award_date_riaa'] >= merged_riaa['award_date_bestnew']) &
                        (merged_riaa['award_date_riaa'] <= merged_riaa['award_date_bestnew'] + pd.DateOffset(years=5))]

# reading the last five columns of the dummy riaa df
last_five_cols = dummy_riaa_df.iloc[:, -5:]

# grouping by artist and adding the values in the last five columns for each artist
riaa_counts = last_five_cols.groupby(five_yr_riaa['artist']).sum()       

# merging the best_new df and RIAA counts 
counts_riaa_five = pd.merge(best_new_df, riaa_counts, on = 'artist', how = 'left')

# renaming columns and filling in missing values with 0

columns = ['diamond', 'gold', 'multi_diamond', 'multi_platinum', 'platinum']

counts_riaa_five[columns] = counts_riaa_five[columns].fillna(0)

# adjusting data type
counts_riaa_five[columns] = counts_riaa_five[columns].astype('int64')

# creating df without the excluded artists
filter_riaa_five = counts_riaa_five[~counts_riaa_five['artist'].isin(excluded_artists)]

filter_riaa_five = filter_riaa_five.reset_index()

filter_riaa_five = filter_riaa_five.drop(columns = 'index')

# totaling the RIAA total score
riaa_score_list = []

for index, row in filter_riaa_five.iterrows():
    artist_score = (row['diamond'] * 4) + (row['gold'] * 1) + (row['multi_diamond'] * 5) + (row['multi_platinum'] * 3) + (row['platinum'] * 2)
    riaa_score_list.append(artist_score)
    
    
filter_riaa_five['riaa_score'] = riaa_score_list

# merging and creating score df

# merging the vmas and amas filtered df 
merge_1 = pd.merge(filter_vmas_five, filter_amas_five, on = 'artist')

merge_1 = merge_1.drop(columns = ['award_date_x', 
                                  'vmas_lost', 
                                  'vmas_win', 
                                  'award_date_y', 
                                  'amas_lost', 
                                  'amas_win'])

# adding the RIAA filtered df to the merged df                                 
merge_2 = pd.merge(merge_1, filter_riaa_five, on = 'artist')

merge_2 = merge_2.drop(columns = ['award_date', 
                                        'diamond', 
                                        'gold', 
                                        'multi_diamond', 
                                        'multi_platinum', 
                                        'platinum'])

# merging the billboard filtered df to the merged df
all_scores = pd.merge(merge_2, filter_billboard_scores, on = 'artist')
all_scores = all_scores.drop(columns = 'award_date')

# Normalizing the scores 

# copying the df
all_scores_2 = all_scores

# I will be using the min-max normlization technique
#finding min and max of vmas_score

vmas_min = all_scores_2['vmas_score'].min()
vmas_max = all_scores_2['vmas_score'].max()

# new df
all_scores_scaled = all_scores_2

#Normalizing the vmas column
all_scores_scaled['vmas_score'] = (all_scores_scaled['vmas_score'] - vmas_min)/(vmas_max - vmas_min)

# min and max of amas
amas_min = all_scores_2['amas_score'].min()
amas_max = all_scores_2['amas_score'].max()

# Normalizing the amas column
all_scores_scaled['amas_score'] = (all_scores_scaled['amas_score'] - amas_min)/(amas_max - amas_min)

# min and max of riaa
riaa_min = all_scores_2['riaa_score'].min()
riaa_max = all_scores_2['riaa_score'].max()

# Normalizing the riaa column
all_scores_scaled['riaa_score'] = (all_scores_scaled['riaa_score'] - riaa_min)/(riaa_max - riaa_min)

# min and max of billboard
billboard_min = all_scores_2['billboard_score'].min()
billboard_max = all_scores_2['billboard_score'].max()

# Normalizing the billboard column
all_scores_scaled['billboard_score'] = (all_scores_scaled['billboard_score'] - billboard_min)/(billboard_max - billboard_min)

# creating total scores column

all_scores_scaled['total_score'] = all_scores_scaled['vmas_score'] + all_scores_scaled['amas_score'] + all_scores_scaled['riaa_score'] + all_scores_scaled['billboard_score']

```

For this project, I decided to base the success of the artist based on these four factors: the number of Billboard Charted songs, RIAA awards, AMAS/VMAS awards and nominations.

In order to take into consideration the fact that older artists may have more activity compared to newer winners, I will only be focusing on each artist’s accomplishments within the five years after they won the Best New Artist award. So in this case the artists from 2019 and on will not be included. 

Here is how I will be scoring the artists:

Billboard single: If chart position is

- 1 - 20: 5 points per song
- 21 - 40: 4 points per song
- 41 - 60: 3 points per song
- 61 - 80: 2 points per song
- 81 - 100: 1 point per song
- featured artist: 1 point per song

RIAA Awards:

- Multi-Diamond: 5 points
- Diamond: 4 points
- Multi-Platinum: 3 points
- Platinum: 2 points
- Gold: 1 points

American Music Awards/MTV Video Music Awards:

- Win: 5 points
- Nomination: 2 points

After calculating the scores for each artist based on each feature, I noticed that some features had a much wider range of values than the others. To make sure all features contributed equally to the final score, I used a method called min-max normalization. This technique scales all scores to fit within the same range. Once the scores were normalized, I added them together to calculate each artist’s total success score. 

``` {python}
# creating df to be used for modeling
# predictors: gender, music group, genre
# target: total success score

# best new artist df with the artists from 2019 and on excluded
best_new_filtered = best_new_df.iloc[:-4].reset_index()
best_new_filtered = best_new_filtered.drop(columns = 'index')

# merging music group and gender from artist_info_df 
artist_info_subset = artist_info_df[['artist','gender', 'music_group']]
modeling_merge_1 = pd.merge(best_new_filtered, artist_info_subset, on = 'artist', how = 'left')

# merging music genre from artist_genres_df
modeling_merge_2 = pd.merge(modeling_merge_1, artist_genres_df, on = 'artist', how = 'left')
modeling_merge_2

# merging the total success scores
all_scores_scaled_subset = all_scores_scaled[['artist', 'total_score']]
final_modeling_merge = pd.merge(modeling_merge_2, all_scores_scaled_subset, on = 'artist', how = 'left')

# generalizing the genre category for easier modeling
# parent genres to define broader categories
parent_genre = {
    'folk': ['folk', 'lilith'], 
    'jazz': ['jazz'],
    'soul': ['soul', 'neo soul', 'pop soul'],
    'pop': ['pop'],
    'rock': ['rock'],
    'r&b': ['r&b'],
    'hip-hop': ['hip-hop', 'freestyle', 'hip hop', 'hip pop'],
    'rap': ['rap'],
    'country': ['country'],
    'metal': ['alternative metal'],
    'mellow gold': ['mellow gold']
}

# function made to categorize genres into broader categories
def categorize_genre(genre):
    for category, keywords in parent_genre.items():
        for keyword in keywords:
            if re.search(r'\b' + re.escape(keyword) + r'\b', genre.lower()):
                return category
    return None  # if no match is found

# list to store the most common general genre for each artist
most_common_genres = []

# iterating over each row of the df
for index, row in final_modeling_merge.iterrows():
    # getting the list of genres associated with the current artist
    genre_list = row['genre'].split(', ')
    
    # grouping genres into broader categories
    categorized_genres = [categorize_genre(genre) for genre in genre_list]
    
    # genres that couldn't be categorized = none
    categorized_genres = [genre for genre in categorized_genres if genre is not None]
    
    # counting occurrences of each categorized genre
    genre_counts = Counter(categorized_genres)
    
    # finding the most common categorized genre and adding to list
    most_common_genre, count = genre_counts.most_common(1)[0] if genre_counts else (None, 0)
    most_common_genres.append(most_common_genre)

# adding the most common general genre column to the df
final_modeling_merge['most_common_general_genre'] = most_common_genres

# dropping artist column since we do not need it for for modeling purposes
# also droping award_date and original genre column
final_modeling_merge = final_modeling_merge.drop(columns = ['artist', 'award_date', 'genre'])

# performing one-hot encoding for the 'genre' column
genre_dummies = final_modeling_merge['most_common_general_genre'].str.get_dummies()

# concatenating the one-hot encoded genre columns with the original DataFrame
final_modeling_merge = pd.concat([final_modeling_merge, genre_dummies], axis=1)

# droping the original 'most_common_general_genre' column
final_modeling_merge.drop(columns=['most_common_general_genre'], inplace=True)

# changing data types, dropping total score column for now
modeling_df_enc = pd.get_dummies(final_modeling_merge).astype(int)
modeling_df_enc = modeling_df_enc.drop(columns = 'total_score')

# stripping ''
modeling_df_enc.columns = modeling_df_enc.columns.str.strip('"')

# adding total score back, renaming to success_score
modeling_df_enc['success_score'] = final_modeling_merge['total_score']

# missing values check 
modeling_df_enc.isna().sum().sum()

# replacing spaces in column headers with '_'
modeling_df_enc.columns = modeling_df_enc.columns.str.replace(' ', '_')
```

To predict the scores I tested several regression models. After combining the necessary information, I checked for multicollinearity between the predictors I will be using: music genres, artist gender, and membership in music groups. Multicollinearity occurs when predictors are very correlated with each other, which can affect the reliability of a model’s predictions. To address this, I dropped five predictors with high multicollinearity scores, leaving only the music genres as the predictors for my model.

I will create and compare the following regression models: Linear Regression, Lasso Regression, Ridge Regression, Support Vector Regression, and Decision Tree Regressor. 

## Modeling Performance
``` {python}
#| warning: false
# predictors
x = modeling_df_enc.drop(columns = ["success_score"],axis=1)
# target variable
y = modeling_df_enc['success_score']

# finding vif values to also check for multicollinearity
vif_vals = pd.DataFrame()
vif_vals['Variable'] = x.columns
vif_vals['VIF'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif_vals[vif_vals['VIF'] > 10] 

# dropping columns from predictors df to decrease overall multicollinearity
x = x.drop(columns = ['gender_both', 'gender_female', 'gender_male', 'music_group_no', 'music_group_yes'],axis=1)

# checkign vif values one more time 
vif_vals = pd.DataFrame()
vif_vals['Variable'] = x.columns
vif_vals['VIF'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif_vals[vif_vals['VIF'] > 10]

# standarizing the predictor values
sc = StandardScaler()
X = sc.fit_transform(x)

#70/30 train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42, test_size=0.30)
```

``` {python}

#| echo : true

# Linear Regression 
linear_model = LinearRegression()

# fitting the model
linear_model.fit(X_train, y_train)
linear_model

# predicting 
y_pred = linear_model.predict(X_test)
y_pred

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('--------------------------')
print(f'Linear Regression:' )
print('--------------------------')
print("Mean Squared Error (MSE) - Linear:", mse)
print("Mean Absolute Error (MAE) - Linear:", mae)      
print("R-squared (R2) - Linear:", r2)

# Lasso Regression 

# testing different alpha values
alphas = [0.001, 0.01, 0.1, 1, 10, 100]

for i in alphas: 
    
    # creating Lasso regression model
    lasso_model = Lasso(alpha = i)  

    # fitting the model
    lasso_model.fit(X_train, y_train)
    
    # predicting
    y_pred_lasso = lasso_model.predict(X_test)
    
    # calculating evaluation metrics
    mse_lasso = mean_squared_error(y_test, y_pred_lasso)
    mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
    r2_lasso = r2_score(y_test, y_pred_lasso)

# Note, I compared the alpha values and 0.001 performed the best
print('---------------------------------')
print(f'Lasso Regression - Alpha = {0.001}:' )
print('---------------------------------')
print("Mean Squared Error (MSE) - Lasso:", mse_lasso)
print("Mean Absolute Error (MAE) - Lasso:", mae_lasso)      
print("R-squared (R2) - Lasso:", r2_lasso)



# Ridge Regression

# testing different alpha values
alphas = [0.001, 0.01, 0.1, 1, 10, 100]

for i in alphas:
    
    # creating the ridge model
    ridge_model = Ridge(alpha = i)
    
    # fitting the model
    ridge_model.fit(X_train, y_train)
    
    # predicting 
    y_pred_ridge = ridge_model.predict(X_test)
    
    # calculating evaluation metrics
    mse_ridge = mean_squared_error(y_test, y_pred_ridge)
    mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
    r2_ridge = r2_score(y_test,y_pred_ridge)

# Note, I compared the alpha values and 0.01 performed the best  
print('---------------------------------')
print(f'Ridge Regression - Alpha = {0.01}:' )
print('---------------------------------')
print("Mean Squared Error (MSE) - Ridge:", mse_ridge)
print("Mean Absolute Error (MAE) - Ridge:", mae_ridge)      
print("R-squared (R2) - Ridge:", r2_ridge)

# Support Vector Regression
svr_reg = Pipeline([
        ("scaler", StandardScaler()),
        ("svr", SVR(kernel = 'linear', C = 1)),
    ])

# fitting the SVR classifier
svr_reg.fit(X_train, y_train)

# predicting
y_pred_svr = svr_reg.predict(X_test)

# calculating evaluation metrics
mse_svr = mean_squared_error(y_test, y_pred_svr)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)

print('--------------------------')
print(f'Support Vector Regression (SVR):' )
print('--------------------------')
print("Mean Squared Error (MSE) - SVR:", mse_svr)
print("Mean Absolute Error (MAE) - SVR:", mae_svr)
print("R-squared (R2) - SVR:", r2_svr)

# Decision Tree
tree_clf = DecisionTreeRegressor(random_state = 0, max_depth = 10)

# fitting the dt to the trainign data
tree_clf.fit(X_train, y_train)

# predicting the target values of the test
y_pred_tree = tree_clf.predict(X_test)

# calculating evaluation metrics
mse_tree = mean_squared_error(y_test, y_pred_tree)
mae_tree = mean_absolute_error(y_test, y_pred_tree)
r2_tree = r2_score(y_test, y_pred_tree)

print('--------------------------')
print(f'Decision Tree Regressor:' )
print('--------------------------')
print("Mean Squared Error (MSE) - Decision Tree:", mse_tree)
print("Mean Absolute Error (MAE) - Decision Tree:", mae_tree)
print("R-squared (R2) - Decision Tree:", r2_tree)
```

After evaluating the performance of each regression model, it is safe to say that the Decision Tree Regressor model outperforms the other. This conclusion is supported by its relatively low Mean Squared Error (MSE) and Mean Absolute Error (MAE) values, suggesting a better predictive accuracy compared to the other models. Furthermore, the Decision Tree Regressor has the highest R-squared value, indicating that it explains a greater proportion of the variance in the target variable. 

However, it is important to recognize that while this model demonstrates the best performance among the models tested, its performance is not that exceptional. This is most likely due to the limited size and simplicity of the data, which provides a constrained number of observations and features. It is reasonable to infer that more data would further strengthen the models’ predictive capabilities.

## Features Impacting Success

Utilizing the Decision Tree Regressor, I went ahead and found the top five features that have a significant impact on an artist's success following their award win.

``` {python }

#| echo : true 

# finding the features that are the most significant in the decison tree's predictions

dt_feature_importance = pd.Series(tree_clf.feature_importances_, index = x.columns)
top_5_dt = dt_feature_importance.nlargest(5)

print("----------------------------------------------")
print("The Top 5 Feature Importance - Decision Tree")
print("----------------------------------------------")
print(top_5_dt)
```

# Conclusions

## Limitations

The biggest limiation I faced while working on this project was lack of data. Not only was I working with a small set of artists, but I was very limited to what data I can collect for these artists. Some information I wanted to, but could not, collect are: number of concerts/tours each artist performed, and mentions in social media. 

Furthermore, success is not only about the numbers and the amount of awards and recognition an artist recieves throughout the years. My project lacks the ability to take into action the more impactful ways artist's have made their marks on the music industry. Success is a very subjective topic, so at the end of the day, an artist may be seen as more successful and impactful than others by certain people. 

It is also worth noting that some celebrities dissappear from the music industry due to personal factors, such as: retirement, scandals, death, etc..

## Final Thoughts

This project was very enjoyable to work on as it allowed me to mix my own personal passions and curiousities with my knowledge in Data Science. I believe this curse is simply a myth, it does not exist. Based on the visuals presented in this project, there are many artists who continue to grow their careers after winning the Best New Artist Award.

In the future, I would love to expand on my findings and dive deeper into the reason why certain artists become more successful than others. To add more observations, I would like to compare the success of the Grammy Best New Artist's to equivalent Best New Artist award's in other music award shows. Furthermore, I am looking to also utilize my models to create a program that will allow one to input features an artist possesses in order to predict their success score.